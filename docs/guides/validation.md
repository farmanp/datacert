# Validation Guide

*Audience: QA engineers and data teams*

## Overview

The **Quality** tab allows you to upload existing validation rules (GX Suites, Soda Checks, JSON Schema) and instantly see which checks pass or fail against your profiled data‚Äî**without running the actual validation tools**.

This is useful for:
- **Pre-deployment checks**: Validate before pushing to production
- **Schema drift detection**: Check if new data meets old rules
- **Quality CI/CD**: Automated validation in pipelines

---

## Quick Start

1. **Profile your data** (Upload ‚Üí Wait for results)
2. Switch to **Quality** tab (top toggle)
3. **Drag and drop** a validation file (`.yml`, `.yaml`, `.json`)
4. View **pass/fail results** instantly

---

## Supported Validation Formats

| Format | File Extension | Example Tools |
|--------|----------------|---------------|
| **Great Expectations** | `.json` | GX Python, GX Cloud |
| **Soda Checks** | `.yml`, `.yaml` | Soda Core, Soda Cloud |
| **JSON Schema** | `.json` | Any JSON validator |

---

## Great Expectations (GX)

### What is GX?

Great Expectations is a Python library for data validation. GX **Suites** are JSON files containing **expectations** (rules) like "column X must be between 0 and 100".

### How Validation Works

DataCert evaluates GX expectations **against your profile statistics**, not raw data:

| Expectation | Evaluated Using | Example |
|-------------|-----------------|---------|
| `expect_column_to_exist` | Column names | ‚úÖ Pass if column exists |
| `expect_column_values_to_not_be_null` | `nullCount / totalRows` | ‚úÖ Pass if ‚â§ threshold |
| `expect_column_values_to_be_between` | `min`, `max` from profile | ‚úÖ Pass if within bounds |
| `expect_column_values_to_be_unique` | `distinctCount == totalRows` | ‚úÖ Pass if all unique |
| `expect_column_mean_to_be_between` | `mean` from profile | ‚úÖ Pass if in range |

### Unsupported Expectations

Some expectations require **raw data access** and will be **skipped**:
- `expect_column_values_to_match_regex` (needs actual values)
- Custom SQL expectations (needs database)

**Status**: ‚ö™ **Skipped** with reason displayed

### Example GX Suite

```json
{
  "expectation_suite_name": "sales_data_suite",
  "expectations": [
    {
      "expectation_type": "expect_column_to_exist",
      "kwargs": { "column": "order_id" }
    },
    {
      "expectation_type": "expect_column_values_to_not_be_null",
      "kwargs": { "column": "order_id", "mostly": 1.0 }
    },
    {
      "expectation_type": "expect_column_values_to_be_between",
      "kwargs": {
        "column": "order_value",
        "min_value": 0,
        "max_value": 10000
      }
    }
  ]
}
```

### Validation Results

**Pass** üü¢:
```
‚úì expect_column_to_exist(order_id)
‚úì expect_column_values_to_not_be_null(order_id)
  Observed: 100% non-null, Expected: 100%
```

**Fail** üî¥:
```
‚úó expect_column_values_to_be_between(order_value)
  Observed: max=15,000, Expected: max‚â§10,000
```

---

## Soda Checks (SodaCL)

### What is Soda?

Soda Core is an open-source data quality tool using **SodaCL** (Soda Checks Language), a YAML-based DSL.

### Supported Checks

| Check Type | Validation Logic | Example |
|------------|------------------|---------|
| `missing_count(col)` | `profile.missingCount` | `<= threshold` |
| `missing_percent(col)` | `(missing / count) * 100` | `< threshold%` |
| `duplicate_count(col)` | `count - distinctCount` | `= 0` |
| `min(col)` | `profile.numericStats.min` | `>= threshold` |
| `max(col)` | `profile.numericStats.max` | `<= threshold` |
| `avg(col)` | `profile.numericStats.mean` | `between X and Y` |
| `row_count` | `profile.totalRows` | `> threshold` |

### Unsupported Checks

- `freshness` (requires timestamp comparison)
- Custom SQL checks (needs database connection)

### Example Soda Checks YAML

```yaml
# Generated by DataCert
# Source: sales_data.csv
# Tolerance: 10%

checks for sales_data:
  # Column: order_id
  - missing_count(order_id) = 0
  - duplicate_count(order_id) = 0

  # Column: order_value
  - missing_percent(order_value) < 5
  - min(order_value) >= 0
  - max(order_value) <= 15000
  - avg(order_value) between 100 and 200

  # Table-level
  - row_count > 1000
```

### Validation Results

**Pass** üü¢:
```
‚úì missing_count(order_id) = 0
‚úì duplicate_count(order_id) = 0
```

**Fail** üî¥:
```
‚úó missing_percent(order_value) < 5
  Observed: 7.2%, Threshold: <5%
```

---

## JSON Schema

### What is JSON Schema?

JSON Schema is a universal standard for describing JSON/data structures. It's language-agnostic and works with many validators.

### How Validation Works

DataCert checks:
- **Type matching**: Does `inferredType` match schema `type`?
- **Required fields**: Are all `required` columns present?
- **Numeric bounds**: Do `min`/`max` fall within schema constraints?
- **String length**: Does `minLength`/`maxLength` match?

### Example JSON Schema

```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "properties": {
    "order_id": {
      "type": "integer",
      "minimum": 1,
      "maximum": 100000
    },
    "email": {
      "type": "string",
      "format": "email",
      "minLength": 5,
      "maxLength": 255
    },
    "order_value": {
      "type": "number",
      "minimum": 0
    }
  },
  "required": ["order_id", "email"]
}
```

### Validation Results

**Pass** üü¢:
```
‚úì order_id (type: integer)
‚úì order_id (required field present)
```

**Fail** üî¥:
```
‚úó email (type mismatch)
  Observed: String, Expected: string with format email
```

---

## Workflow Examples

### Workflow 1: Pre-Deployment Validation

**Scenario**: Check if new batch meets production rules

1. **Generate rules** from reference dataset:
   - Profile golden dataset ‚Üí Export GX Suite (10% tolerance)
2. **Validate new data**:
   - Profile new batch ‚Üí Quality tab ‚Üí Upload GX Suite
3. **Review results**:
   - All pass? ‚úÖ Deploy
   - Failures? üî¥ Investigate why data changed

### Workflow 2: Schema Drift Detection

**Scenario**: Detect when upstream schema changes

1. **Baseline schema**:
   - Profile v1 data ‚Üí Export JSON Schema
2. **Daily validation**:
   - Profile new daily files ‚Üí Quality tab ‚Üí Upload schema
3. **Alert on drift**:
   - New column? Schema validation fails
   - Type changed? Validation fails

### Workflow 3: CI/CD Integration

**Scenario**: Automated quality gates

```bash
# 1. Profile new data (headless)
npx playwright test profile-data.spec.ts

# 2. Upload validation rules via API
curl -X POST /validate \
  -F "profile=@profile.json" \
  -F "rules=@gx_suite.json"

# 3. Fail build if >10% checks fail
if [[ $FAIL_RATE > 0.1 ]]; then
  exit 1
fi
```

---

## UI Guide

### Validation Results Panel

**Summary Cards:**
- **Passed**: Green count of successful checks
- **Failed**: Red count of failing checks
- **Skipped**: Gray count of unsupported checks

**Result List:**
Each check shows:
- **Status icon**: ‚úÖ Pass, ‚ùå Fail, ‚ö™ Skip
- **Column name** (or "Table Level")
- **Check type** (e.g., `missing_count`, `expect_column_to_exist`)
- **Observed vs Expected** (for failures)
- **Skip reason** (for unsupported checks)
- **Raw check** (hover to see original rule)

### Uploading Multiple Files

You can upload **multiple validation files** sequentially:
- Each creates a **new result card**
- Previous results stay visible
- Click **√ó** to remove a result

---

## Tips & Troubleshooting

### "No profile data found"
**Cause**: You haven't profiled data yet  
**Fix**: Go back to main view, upload a file, wait for profiling

### "Unknown JSON format"
**Cause**: JSON file doesn't match GX/JSON Schema structure  
**Fix**: Verify file is a valid GX Suite or JSON Schema

### "Cannot validate: requires raw data"
**Cause**: Check needs actual values (e.g., regex patterns)  
**Fix**: Run full GX/Soda scan on database for comprehensive validation

### Many skipped checks
**Cause**: Complex rules beyond profile statistics  
**Fix**: DataCert validates **what it can**‚Äîuse full tools for 100% coverage

---

## Limitations

DataCert validation is **profile-based**, not **data-based**:

‚úÖ **Can validate:**
- Type checks
- Null counts
- Numeric bounds (min/max/mean)
- Uniqueness
- Row counts

‚ùå **Cannot validate:**
- Regex patterns
- Custom SQL logic
- Freshness (time-based)
- Cross-table joins
- Complex business rules

**Use case**: DataCert is for **fast feedback**. For production validation, use the full GX/Soda/jsonschema tools.

---

## Next Steps

- **[Export Guide](./exports.md)** ‚Äì Generate validation rules from your data
- **[Profiling Guide](./profiling.md)** ‚Äì Understand profile statistics
